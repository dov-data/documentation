Scope

Database objects including tables, views, triggers, and stored procedures should all adhere to a basic set of standards. These standards should be developed with portability, legibility, and ease of use in mind. All objects should be modeled as closely to best practices as feasible. Special care and judgment should be used when drifting from best practice.

Naming Conventions

Naming conventions are not a performance impacting standard in the normal sense, however, a lack of consistency in naming leads to frequently repeated mistakes and alternatives methods of handling (i.e. casting all objects to upper or lower).

Objects standards

no use of reserved words
all lower case
underscore separation (snake_case), mixed case (camelCase), and proper (PascalCase) notations are all fine, but choose one and stick with it for all naming. Hyphenated (kebab-case) is not recommended as the '-' is a reserved math operator and as a result requires back tick notation.
last noun plurality (i.e. books, book_titles)
Regarding reserved words, if an object requires back-ticks to be referenced, that should be considered a major red flag. Pro-tip: use a syntax highlighting editor, reserved words are colorized. The full list can be found https://dev.mysql.com/doc/refman/8.0/en/keywords.html.

Tables Standards

all tables will have an auto-incrementing primary key as table_name_id*
column names follow standard naming convention however there is no plurality (i.e. book_titles.title_character_count as opposed title_characters)
use appropriate datatype
avoid duplicate column names
*cross-reference (xref) tables can have a composite primary key made up of the primary keys of the associated tables

DATATYPES

 NUMERIC
 
 Datatype mismatching and over-provisioning have consequences to read and write operations. It is inappropriate to type every numeric datatype as an INT. Unsigned integers can reach values of 4294977295 or almost 4.3 billion unique values. This data takes 4 bytes. Using INT to represent a boolean is inefficient and costly. A TINYINT, by contrast, is 1 byte and its max unsigned value is 255 which is much more appropriate for a field used in this fashion. A field storing the employee number at a business might be typed a MEDIUMINT for example with a max unsigned value of ~16.7 million. It is unlikely any company will hire more employees than that. Different storage consumption affects the hardware layer as it impacts how many rows can be buffered in memory and how many records can be written to physical disk pages. If you are concerned that an auto-incremented ID field could be exhausted and feel the need to just go with the next larger 'just in case', please keep in mind that until we are trying to alter an id from INT to BIGINT, we are only talking about 16.7 million rows (less because it will be noted prior to getting that close) and an ALTER can be done without much impact.
 
Any time monetary or scientific precision is needed the only acceptable is datatype is DECIMAL, the similar datatypes FLOAT and DOUBLE (synonyms) are 'fuzzy' precision, that is to say, they round and the exact precision is not known. A DOUBLE(4,1) can be 4 integers in total, 1 after the decimal point, so inserting 5.09 will round to 5.1, because of this, you cannot treat 5.1 as = to 5.1 in a DOUBLE (FLOAT). This data type is appropriate for test scores perhaps.
 The BIT data type is a binary string. This data type is useful in micro deployments where hardware limitations are imposed. This is meant for very low-level programmatic operations. This data type is not very useful for and not appropriate for web/graphic based applications. This data type would be used where memory and CPU are very limited ala embedded services.
 
 As a general rule specify integers as unsigned unless negative values are expected. This applies to fixed and floating-point data types as well, but the use cases for negative values is much higher here (negative account balance, or percentage decrease for example). Auto incremented fields should always be specified NOT NULL and UNSIGNED.
 
 DATE and TIME
 
 MySQL as an engine has long held the philosophy that is a storage and retrieval engine first and foremost. A consequence of this is that the engine has always allowed bad data storage, this includes invalid dates (GIGO). It may convert an invalid date silently, or it may store a totally invalid date. Care should be taken to ensure this does not happen. The engine can be configured to disallow zero date values but this is not enabled by default MySQL should never be relied on to manage date validity. Date and times can be compared numerically without casting the value to an integer, or unixtime. This is a native feature. Likewise, MySQL interprets several formats for comparison and write. For example date_field = '03:06:22' is interpreted as '2003-06-22'.
 
The date and time datatypes are date, time, datetime, timestamp, and year. They are mostly self-explanatory. The format for DATETIME and TIMESTAMP is 'YYYY-MM-DD HH:mm:SS.millis', DATE, TIME, and YEAR are the logical slices of this format. In fact DATE, TIME, and YEAR are keywords that can be used to select just that value from a DATETIME or TIMESTAMP. 'SELECT YEAR(datetime_field) FROM TABLE;' is perfectly compliant syntax. Milliseconds are enabled when defining the fields by setting the precision. DATETIME(6) for example sets precision to 6 places, TIME(0) disables fractional times. Disabled precision is the default if a positive integer is not provided.

DATETIME and TIMESTAMP are functionally the same BUT they are written to disk differently. TIMESTAMP is converted to UTC before storage DATETIME is not. This means that TIMESTAMP values are not portable. If the data is restored on a host whose timezone is not the same as the source, the data will be converted to the WRONG DATE and TIME. Use of the TIMESTAMP data type is heavily discouraged. As a consequence timezone in the database and on the server must be UTC and conversions done outside the data storage layer of the stack. If data is not stored UTC, it suffers double writes at the end of daylight savings time and a missing hour at the start. Timezone can be defined at the connection level as well. These issues make reconciliation very difficult if not impossible. Use DATETIME and UTC in all things. DATETIME and TIMESTAMP can be defined to automatically set on create, update on change, or both.

The TIME data type can be used in comparisons. An example would be estimating battery life or showing the hours elapsed since a test was last run. TIME values can be negative.

STRINGS

 Strings datatypes are CHAR and VARCHAR, BINARY and VARBINARY, BLOB and TEXT, ENUM, and the SET.
 
The most commonly used data types are CHAR and VARCHAR. These store character strings. The types differ in that CHAR are padded to the defined length whereas VARCHAR stores the ( (number of characters*bytes per character) + (1 byte to record length) ) bytes. Ergo, CHAR(255) takes 255 bytes even if the actual value is 'foo'; VARCHAR, by contrast, would take 4 bytes, 3 for 'foo' and one for the length. Character set affects length as some can be as much as 4 bytes per character as in utf8mbf. Character set choice impacts not just the amount of data stored on disk, but in memory as well. The potential for performance gains by using a small character set by default and an expanded only on certain columns is usually not massive and is outweighed by the potential to mix sets from table to table on fields that are compared. Comparing fields with different character sets will cause a conversion by the MySQL optimizer and this conversion will negate the use of indexes. In light of this is, it is strongly recommended that a single character set and collation is employed.

The BINARY and VARBINARY are similar to the CHAR and VARCHAR data types and store similar data. The BINARY and VARBINARY store the data in binary form as opposed to character. They contain byte strings rather than character. These data types are case sensitive when searching. The difference in storage is also applied to sorting operation, these datatypes are always sorted based on the byte order, not the character collation.

 BLOB and TEXT are large objects (LOBs), BLOB are binary and TEXT character strings. Both are divided into four size divisions. They are TINYBLOB, BLOB, MEDIUMBLOB, and LONGBLOB, and likewise TINYTEXT, TEXT, MEDIUMTEXT, LONGTEXT. Often times large reports in pdf or xls, or photographs are stored as BLOB, and large white papers or magazine articles as TEXT. These data types should be used very sparingly if at all. These objects often are larger than a single page and therefore create linked disk pages to create a single row, this causes multiple disk seeks and reads to disk in order to deliver a single row. Employing a clustered filesystem solution and storing pointers in the database is a much better solution.
 
 ENUM is a special data type that is an enumerated set. It is a set of predefined values that are the only choices permissible for this column. For example, a column containing poll results might have a set like ('Definitely No', 'Probably No', 'Undecided', 'Probably Yes', 'Definitely Yes') where the actual values are a 1 through 5 (0 is reserved for empty string). This is a very convenient method of mimicking cross-reference table functionality without the cost of a table join. This data type allows for complex strings to be stored as integer values on disk reducing the average row size and conserving resources. The are caveats to the use of the ENUM. Adding, or removing a potential value from the enumerated set can be problematic and requires a good deal of planning, this is an operation that causes a table rewrite. ENUM can contain 65,535 values, but if you find a need for more than 64 elements, it is better to create a cross-reference table. Integers should NEVER be part of the ENUM. NEVER. If you find yourself contemplating adding an ENUM with potential values of (1,2,3,4,N), you need to rethink the methodology. ENUM values are stored as integers and as a result, are not ordered by any character set. A select ordered by an ENUM with of values ('zebra','aardvark','wolf','badger') will return all the 'zebra' rows first. This limitation can be overcome by using the CAST function, but that invalidates the use of an index in comparisons. ENUM should store at least 3 potential values. In general, the commonly stated best practice is to avoid ENUM unless you are certain the values are very unlikely to change and the values will not have distinct attributes of their own. For the latter consider a books table, you may have a 'fiction_genre' ENUM field with values ('Horror', 'Romance', 'Sci-Fi', 'Fantasy', 'Historical'), etc., what if you want to further distinguish between Ancient and Modern Historical fiction? The model is too rigid for that. The last thing to consider is verbose comments on ENUM columns, one of the benefits of the datatype is that it is somewhat self-documenting, but the downside is that you are allowing for potential misunderstanding of the meaning of a value.
 
 The SET data type is very similar to the ENUM at first glance but has some major distinctions. The similarity is in the appearance of the field definition. The column is defined with a set list of values as is the ENUM, but this data type allows for multiple selections. For example, consider a table of musicians, you may have a SET like instrument_families with possible values of ('Brass','Percussion','Strings','Woodwinds'). This would allow a record for someone who plays the piano (percussion) and clarinet (woodwinds) to exist as a single row. Another difference is the number of members of a SET, only 64 elements are allowed. The biggest difference is in the sorting, SET columns can be assigned a character set and collation, this can be a binary collation to account for letter case. Aside from sorting the same caveats that apply to ENUM apply to SET.

